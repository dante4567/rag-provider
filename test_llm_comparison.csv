Name,Model Type,Parameters (B),Training Data,Performance Score,Use Case
GPT-3,Autoregressive,175,CommonCrawl + Books,85.2,Text Generation
BERT-Large,Bidirectional Encoder,0.34,BookCorpus + Wikipedia,87.1,Text Understanding
T5-Large,Encoder-Decoder,0.77,C4 Dataset,86.8,Text-to-Text
RoBERTa,Bidirectional Encoder,0.35,Extended Training Data,87.5,Robust Understanding
GPT-4,Multimodal,1000+,Diverse Internet Data,92.1,General Purpose
LLaMA-7B,Autoregressive,7,Diverse Sources,78.9,Efficient Generation
PaLM,Autoregressive,540,High-quality Text,88.6,Few-shot Learning
ChatGPT,Fine-tuned GPT,175,Human Feedback,89.3,Conversational AI
Claude-2,Constitutional AI,Unknown,Filtered Internet,88.9,Safe AI Assistant
Alpaca,Instruction-tuned,7,Self-instruct Data,76.4,Instruction Following
Vicuna,Chat Fine-tuned,13,ShareGPT Data,82.1,Open-source Chat
Falcon-40B,Autoregressive,40,RefinedWeb,84.7,Multilingual
MPT-7B,Autoregressive,7,RedPajama Dataset,77.8,Commercial Use
