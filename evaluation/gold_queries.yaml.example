# Gold Query Evaluation Set
#
# Purpose: Measure retrieval quality with precision@k, recall@k, and MRR metrics
# Format: Each query has expected relevant documents for evaluation
#
# How to use:
#   1. Copy this file to gold_queries.yaml
#   2. Replace with your actual ingested documents
#   3. Run: make test-quality (or python scripts/evaluate_retrieval.py)
#
# Metrics explained:
#   - Precision@k: % of retrieved docs that are relevant (quality of results)
#   - Recall@k: % of relevant docs that were retrieved (coverage)
#   - MRR (Mean Reciprocal Rank): Average of 1/rank of first relevant doc
#   - Any Good Citation Rate: % of queries where at least 1 relevant doc appears

queries:
  # Example: School enrollment queries
  - id: "Q001"
    query: "What documents are needed for school enrollment?"
    description: "User wants to know required documents for Anna's school registration"
    expected_docs:
      - "enrollment_application_anna_lins.txt"
      - "school_requirements_grundschule.pdf"
    tags: ["education", "enrollment", "requirements"]
    difficulty: "easy"  # easy | medium | hard

  - id: "Q002"
    query: "When is the deadline for submitting Anna's enrollment?"
    description: "User needs to know specific deadline"
    expected_docs:
      - "enrollment_application_anna_lins.txt"
    tags: ["education", "deadline", "dates"]
    difficulty: "easy"

  - id: "Q003"
    query: "Who should I contact about kindergarten enrollment in Köln?"
    description: "User looking for contact person for school enrollment"
    expected_docs:
      - "enrollment_application_anna_lins.txt"
      - "school_contacts_koeln.txt"
    tags: ["education", "contacts", "administration"]
    difficulty: "medium"

# Evaluation Configuration
config:
  # How many results to retrieve per query (k values to test)
  k_values: [1, 3, 5, 10]

  # Minimum acceptable scores (quality gates)
  min_precision_at_5: 0.60  # At least 60% of top 5 results should be relevant
  min_recall_at_10: 0.70    # Should find at least 70% of relevant docs in top 10
  min_mrr: 0.50             # First relevant doc should average rank 2 or better
  min_any_good_citation: 0.80  # 80% of queries should have ≥1 relevant result

  # Test modes
  use_reranking: true       # Test with cross-encoder reranker enabled
  test_bm25_only: false     # Compare against BM25-only baseline
  test_dense_only: false    # Compare against dense-only baseline

# Notes for creating good gold queries:
#
# 1. **Diversity**: Cover different query types (factual, temporal, procedural, verification, relationship)
# 2. **Difficulty levels**: Easy (direct match), Medium (requires reasoning), Hard (multi-hop/temporal)
# 3. **Real user questions**: Base queries on actual user needs
# 4. **Expected docs**: Only list truly relevant documents (be strict!)
# 5. **Maintenance**: Update monthly as you ingest new documents
